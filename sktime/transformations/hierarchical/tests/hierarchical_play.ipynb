{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# just for plots\n",
    "import plotly.express as px\n",
    "\n",
    "from sktime.datatypes import get_examples\n",
    "from sktime.transformations.hierarchical.reconcile import reconciler\n",
    "\n",
    "# https://otexts.com/fpp3/hierarchical.html\n",
    "# https://github.com/robjhyndman/reconciliation_review_talk/blob/main/10years_reconciliation.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_examples(mtype=\"pd_multiindex_hier\", as_scitype=\"Hierarchical\")\n",
    "df = df[0]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_hierarchy(df_hier, flatten_single_levels=True):\n",
    "    \"\"\"From hierarchical mtype get the full aggregate hierarchy before forecasting\"\"\"\n",
    "\n",
    "    if df_hier.index.nlevels >= 2:\n",
    "        hier_names = list(df_hier.index.names)\n",
    "\n",
    "        # top level\n",
    "        # remove aggregations that only have one level from below\n",
    "        if flatten_single_levels:\n",
    "            single_df = df_hier.groupby([\"timepoints\"]).count()\n",
    "            mask1 = (\n",
    "                single_df[(single_df > 1).all(1)]\n",
    "                .index.get_level_values(\"timepoints\")\n",
    "                .unique()\n",
    "            )\n",
    "            mask1 = df_hier.index.get_level_values(\"timepoints\").isin(mask1)\n",
    "            top = df_hier.loc[mask1].groupby(level=[\"timepoints\"]).sum()\n",
    "        else:\n",
    "            top = df_hier.loc[mask1].groupby(level=[\"timepoints\"]).sum()\n",
    "\n",
    "        ind_names = list(set(hier_names).difference([\"timepoints\"]))\n",
    "        for i in ind_names:\n",
    "            top[i] = \"__total\"\n",
    "\n",
    "        top = top.set_index(ind_names, append=True).reorder_levels(hier_names)\n",
    "\n",
    "        df_out = pd.concat([top, df_hier])\n",
    "\n",
    "        # if we have a hierarchy with mid levels\n",
    "        if len(hier_names) > 2:\n",
    "            for i in range(len(hier_names) - 2):\n",
    "                # list of levels to aggregate\n",
    "                agg_levels = hier_names[0 : (i + 1)]\n",
    "                agg_levels.append(\"timepoints\")\n",
    "\n",
    "                # remove aggregations that only have one level from below\n",
    "                if flatten_single_levels:\n",
    "                    single_df = df_hier.groupby(level=agg_levels).count()\n",
    "                    # get index masks\n",
    "                    masks = []\n",
    "                    for i in agg_levels:\n",
    "                        m1 = (\n",
    "                            single_df[(single_df > 1).all(1)]\n",
    "                            .index.get_level_values(i)\n",
    "                            .unique()\n",
    "                        )\n",
    "                        m1 = df_hier.index.get_level_values(i).isin(m1)\n",
    "                        masks.append(m1)\n",
    "                    mid = (\n",
    "                        df_hier.loc[np.logical_and.reduce(masks)]\n",
    "                        .groupby(level=agg_levels)\n",
    "                        .sum()\n",
    "                    )\n",
    "                else:\n",
    "                    mid = df_hier.groupby(level=agg_levels).sum()\n",
    "\n",
    "                # now fill in index\n",
    "                ind_names = list(set(hier_names).difference(agg_levels))\n",
    "                for j in ind_names:\n",
    "                    mid[j] = \"__total\"\n",
    "                # set back in index\n",
    "                mid = mid.set_index(ind_names, append=True).reorder_levels(hier_names)\n",
    "                df_out = pd.concat([df_out, mid])\n",
    "\n",
    "        df_out.sort_index(inplace=True)\n",
    "        return df_out\n",
    "    else:\n",
    "        return df_hier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the full forecasting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_hierarchy(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test with bottom levels that span two nodes\n",
    "\n",
    "- i.e. mid levels that are only present at a subset of bottom nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"foo\", \"foo2\", \"bar\", \"timepoints\"] + [f\"var_{i}\" for i in range(2)]\n",
    "\n",
    "Xlist = [\n",
    "    pd.DataFrame(\n",
    "        [[\"a\", \"a1\", 0, 0, 1, 4], [\"a\", \"a1\", 0, 1, 2, 5], [\"a\", \"a1\", 0, 2, 3, 6]],\n",
    "        columns=cols,\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        [[\"a\", \"a1\", 1, 0, 1, 4], [\"a\", \"a1\", 1, 1, 2, 55], [\"a\", \"a1\", 1, 2, 3, 6]],\n",
    "        columns=cols,\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        [[\"a\", \"a2\", 2, 0, 1, 42], [\"a\", \"a2\", 2, 1, 2, 5], [\"a\", \"a2\", 2, 2, 3, 6]],\n",
    "        columns=cols,\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        [[\"b\", \"b1\", 0, 0, 1, 4], [\"b\", \"b1\", 0, 1, 2, 5], [\"b\", \"b1\", 0, 2, 3, 6]],\n",
    "        columns=cols,\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        [[\"b\", \"b2\", 1, 0, 1, 4], [\"b\", \"b2\", 1, 1, 2, 55], [\"b\", \"b2\", 1, 2, 3, 6]],\n",
    "        columns=cols,\n",
    "    ),\n",
    "    pd.DataFrame(\n",
    "        [[\"b\", \"b2\", 2, 0, 1, 42], [\"b\", \"b2\", 2, 1, 2, 5], [\"b\", \"b2\", 2, 2, 3, 6]],\n",
    "        columns=cols,\n",
    "    ),\n",
    "]\n",
    "X = pd.concat(Xlist)\n",
    "X = X.set_index([\"foo\", \"foo2\", \"bar\", \"timepoints\"])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note flatten single levels is the default option\n",
    "\n",
    "- see that `(a, a2, 2, *)` and `(b, b1, 0, *)` don't contain `__total`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_hierarchy(X, flatten_single_levels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Example\n",
    "\n",
    "Let's generate a hierarchical dataset similar to the last example from the flights dataset\n",
    "\n",
    "- Generate dataset\n",
    "- Generate full hierarchy\n",
    "- Forecast each level\n",
    "- Reconcile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_airline\n",
    "from sktime.utils.plotting import plot_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1 = load_airline()\n",
    "\n",
    "zone1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for visualization\n",
    "plot_series(\n",
    "    zone1,\n",
    "    10 + zone1 * 5,\n",
    "    -50 + zone1 * 0.9,\n",
    "    zone1 ** 1.5,\n",
    "    -20 + 10 * zone1,\n",
    "    10 + (10 * zone1) + (0.05 * (zone1 ** 2)),\n",
    "    labels=[\"zone1\", \"zone2\", \"zone3\", \"zone4\", \"zone5\", \"zone6\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zone1, index=zone1.index).rename(\n",
    "    columns={\"Number of airline passengers\": \"zone1\"}\n",
    ")\n",
    "\n",
    "df[\"zone2\"] = 10 + zone1 * 5\n",
    "df[\"zone3\"] = zone1 * 0.9 - 50\n",
    "df[\"zone4\"] = zone1 ** 1.5\n",
    "df[\"zone5\"] = zone1 * 10 - 500\n",
    "df[\"zone6\"] = 10 + (10 * zone1) + (0.05 * (zone1 ** 2))\n",
    "\n",
    "df = (\n",
    "    df.melt(ignore_index=False)\n",
    "    .set_index([\"variable\", df.melt(ignore_index=False).index])\n",
    "    .rename_axis([\"airport\", \"timepoints\"], axis=0)\n",
    "    .rename(columns={\"value\": \"passengers\"})\n",
    ")\n",
    "\n",
    "# df['country'] = \"USA\"\n",
    "df.loc[\n",
    "    df.index.get_level_values(level=\"airport\").isin([\"zone1\", \"zone2\", \"zone3\"]),\n",
    "    \"state\",\n",
    "] = \"CA\"\n",
    "df.loc[\n",
    "    df.index.get_level_values(level=\"airport\").isin([\"zone1\", \"zone2\"]), \"city\"\n",
    "] = \"LA\"\n",
    "df.loc[df.index.get_level_values(level=\"airport\").isin([\"zone3\"]), \"city\"] = \"SF\"\n",
    "\n",
    "\n",
    "df.loc[\n",
    "    df.index.get_level_values(level=\"airport\").isin([\"zone4\", \"zone5\", \"zone6\"]),\n",
    "    \"state\",\n",
    "] = \"NY\"\n",
    "df.loc[\n",
    "    df.index.get_level_values(level=\"airport\").isin([\"zone4\", \"zone5\"]), \"city\"\n",
    "] = \"NYC\"\n",
    "df.loc[df.index.get_level_values(level=\"airport\").isin([\"zone6\"]), \"city\"] = \"BF\"\n",
    "\n",
    "df = df.set_index([\"state\", \"city\", df.index])\n",
    "df\n",
    "\n",
    "\n",
    "# df.droplevel(level=-1).index.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate full hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fh = aggregate_hierarchy(df, flatten_single_levels=True)\n",
    "\n",
    "df_fh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast each level\n",
    "\n",
    "here we will forecast each unique level outside `timepoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "\n",
    "# from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "# from sktime.performance_metrics.forecasting import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = df_fh.droplevel(level=\"timepoints\").index.unique()\n",
    "\n",
    "model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set up loop for forecasting\n",
    "# # for i in model_ids:\n",
    "# mods = {}\n",
    "# prds = {}\n",
    "\n",
    "# for i in model_ids:\n",
    "#     # i = model_ids[0]\n",
    "#     y_train, y_test = temporal_train_test_split(df_fh.loc[i], test_size=36)\n",
    "#     fh = ForecastingHorizon(y_test.index, is_relative=False)\n",
    "#     forecaster = ExponentialSmoothing(trend=\"add\", seasonal=\"additive\", sp=12)\n",
    "#     mods[i] = forecaster.fit(y_train)\n",
    "#     prds[i] = forecaster.predict(fh)\n",
    "#     # plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])\n",
    "#     print(i)\n",
    "#     print(mean_absolute_percentage_error(y_test, prds[i], symmetric=True))\n",
    "# prds = (\n",
    "#     pd.concat(prds)\n",
    "#     .rename_axis(df_fh.index.names, axis=0)\n",
    "#     .rename(columns={\"passengers\": \"y_pred\"})\n",
    "# )\n",
    "\n",
    "# # join with meas\n",
    "# prds = pd.concat([prds, df_fh], axis=1, join=\"inner\").rename(\n",
    "#     columns={\"passengers\": \"y_true\"}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i in model_ids:\n",
    "# mods = {}\n",
    "# prds = {}\n",
    "\n",
    "# for i in model_ids:\n",
    "# i = model_ids[0]\n",
    "# y_train, y_test = temporal_train_test_split(df_fh, test_size=36)\n",
    "fh = ForecastingHorizon([1, 2, 3, 4, 5, 6], is_relative=True)\n",
    "forecaster = ExponentialSmoothing(trend=\"add\", seasonal=\"additive\", sp=12)\n",
    "mods = forecaster.fit(df_fh)\n",
    "prds = forecaster.predict(fh)\n",
    "# plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])\n",
    "# print(i)\n",
    "# print(mean_absolute_percentage_error(df_fh, prds, symmetric=True))\n",
    "prds\n",
    "\n",
    "# prds.index = prds.index.rename(['state', 'city', 'airport', 'timepoints'])\n",
    "\n",
    "# prds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconcile - Bottom Up\n",
    "\n",
    "Bottom up is easy we just sum the bottome levels much like aggregate function.\n",
    "\n",
    "But we want it to be compatible with other methods which go like\n",
    "    \n",
    "    - get y 'base' forecasts for all series (previous section)\n",
    "    - get S matrix from df index (defined by hierarchy structure)\n",
    "    - get G matrix for recon (defined by recon method)\n",
    "    - reconcile forecasts - SGy (all methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = reconciler(method=\"bu\")\n",
    "\n",
    "fitted_transfrom = transformer.fit(X=prds[[\"passengers\"]])\n",
    "\n",
    "fitted_transfrom.s_matrix\n",
    "\n",
    "# https://stackoverflow.com/questions/54307300/what-causes-indexing-past-lexsort-depth-warning-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_transfrom.g_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prds[\"y_recon_bu\"] = fitted_transfrom.transform(X=prds[[\"passengers\"]])\n",
    "\n",
    "prds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prds.loc[prds.index.get_level_values(level=-1) == \"1961-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS reconciliation\n",
    "\n",
    "    - Now all we need is the new g_matrix method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_ols = reconciler(method=\"ols\")\n",
    "\n",
    "fitted_transfrom_ols = transformer_ols.fit(X=prds[[\"passengers\"]])\n",
    "\n",
    "fitted_transfrom_ols.g_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prds[\"y_recon_ols\"] = fitted_transfrom_ols.transform(X=prds[[\"passengers\"]])\n",
    "\n",
    "prds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work fine as well\n",
    "\n",
    "    - note the bottom level forecasts have now changed as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prds.loc[prds.index.get_level_values(level=-1) == \"1961-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WLS structural reconciliation\n",
    "\n",
    "    - Now all we need is the new g_matrix method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_wls = reconciler(method=\"wls_str\")\n",
    "\n",
    "fitted_transfrom_wls = transformer_wls._fit(X=prds[[\"passengers\"]])\n",
    "\n",
    "fitted_transfrom_wls.g_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prds[\"y_recon_wls\"] = fitted_transfrom_wls._transform(X=prds[[\"passengers\"]])\n",
    "\n",
    "prds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prds.loc[prds.index.get_level_values(level=-1) == \"1961-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test different hierarchy set ups\n",
    "\n",
    "- set up work for different hierarchies\n",
    "- first we need a data generating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottom_hier_datagen(\n",
    "    no_levels=3,\n",
    "    no_bottom_nodes=6,\n",
    "    intercept_max=20,\n",
    "    coef_1_max=20,\n",
    "    coef_2_max=0.1,\n",
    "    power_2=[0.5, 1, 1.5, 2],\n",
    "):\n",
    "\n",
    "    if no_levels > no_bottom_nodes:\n",
    "        raise ValueError(\"no_levels should be less than no_bottom_nodes\")\n",
    "\n",
    "    base_ts = load_airline()\n",
    "    df = pd.DataFrame(base_ts, index=base_ts.index)\n",
    "    df.index.rename(None, inplace=True)\n",
    "\n",
    "    if no_levels == 0:\n",
    "        df.columns = [\"passengers\"]\n",
    "        df.index.rename(\"timepoints\", inplace=True)\n",
    "        return df\n",
    "    else:\n",
    "\n",
    "        df.columns = [\"l1_node01\"]\n",
    "\n",
    "        intercept = np.arange(0, intercept_max, 0.01)\n",
    "        coef_1 = np.arange(0, coef_1_max, 0.01)\n",
    "        coef_2 = np.arange(0, coef_2_max, 0.01)\n",
    "\n",
    "        node_lookup = pd.DataFrame(\n",
    "            [\"l1_node\" + f\"{x:02d}\" for x in range(1, no_bottom_nodes + 1)]\n",
    "        )\n",
    "        node_lookup.columns = [\"l1_agg\"]\n",
    "\n",
    "        if no_levels >= 2:\n",
    "\n",
    "            for i in range(2, no_levels + 1):\n",
    "                node_lookup[\"l\" + str(i) + \"_agg\"] = node_lookup.groupby(\n",
    "                    [\"l\" + str(i - 1) + \"_agg\"]\n",
    "                )[\"l1_agg\"].transform(\n",
    "                    lambda x: \"l\"\n",
    "                    + str(i)\n",
    "                    + \"_node\"\n",
    "                    + f\"{int(np.sort(np.random.choice(np.arange(1, np.floor(len(node_lookup.index)/i)+1, 1), size=1))):02d}\"  # noqa from flake8 E501\n",
    "                )\n",
    "\n",
    "        node_lookup = node_lookup.set_index(\"l1_agg\", drop=True)\n",
    "\n",
    "        for i in range(2, no_bottom_nodes + 1):\n",
    "            df[\"l1_node\" + f\"{i:02d}\"] = (\n",
    "                np.random.choice(intercept, size=1)\n",
    "                + np.random.choice(coef_1, size=1) * df[\"l1_node01\"]\n",
    "                + (\n",
    "                    np.random.choice(coef_2, size=1)\n",
    "                    * (df[\"l1_node01\"] ** np.random.choice(power_2, size=1))\n",
    "                )\n",
    "            )\n",
    "\n",
    "        df = (\n",
    "            df.melt(ignore_index=False)\n",
    "            .reset_index(drop=False)\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"variable\": \"l1_agg\",\n",
    "                    \"index\": \"timepoints\",\n",
    "                    \"value\": \"passengers\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        df = pd.merge(left=df, right=node_lookup.reset_index(), on=\"l1_agg\")\n",
    "        df = df[df.columns.sort_values(ascending=True)]\n",
    "\n",
    "        df_newindex = [\"l\" + str(x) + \"_agg\" for x in range(1, no_levels + 1)][::-1]\n",
    "        df_newindex.append(\"timepoints\")\n",
    "\n",
    "        df = df.set_index(df_newindex)\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "np.random.seed(4)\n",
    "df = bottom_hier_datagen(no_bottom_nodes=20, no_levels=3)\n",
    "\n",
    "df[\"node_id\"] = [\"_\".join(x) for x in list(df.droplevel(-1).index)]\n",
    "\n",
    "fig = px.scatter(\n",
    "    data_frame=df,\n",
    "    x=df.index.get_level_values(-1).to_timestamp(),\n",
    "    y=\"passengers\",\n",
    "    color=\"node_id\",\n",
    ")\n",
    "fig.update_traces(marker={\"size\": 5})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = aggregate_hierarchy(df, flatten_single_levels=True)\n",
    "df[\"node_id\"] = [\"_\".join(x) for x in list(df.droplevel(-1).index)]\n",
    "\n",
    "fig = px.scatter(\n",
    "    data_frame=df,\n",
    "    x=df.index.get_level_values(-1).to_timestamp(),\n",
    "    y=\"passengers\",\n",
    "    color=\"node_id\",\n",
    ")\n",
    "fig.update_traces(marker={\"size\": 5})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "df = bottom_hier_datagen(no_bottom_nodes=15, no_levels=4)\n",
    "\n",
    "df = aggregate_hierarchy(df, flatten_single_levels=True)\n",
    "df\n",
    "\n",
    "fh = ForecastingHorizon([1, 2, 3, 4, 5, 6], is_relative=True)\n",
    "forecaster = ExponentialSmoothing(trend=\"add\", seasonal=\"additive\", sp=12)\n",
    "mods = forecaster.fit(df[[\"passengers\"]])\n",
    "prds = forecaster.predict(fh)\n",
    "prds\n",
    "\n",
    "transformer = reconciler(method=\"bu\")\n",
    "fitted_transfrom = transformer.fit(X=prds[[\"passengers\"]])\n",
    "prds[\"y_recon_bu\"] = fitted_transfrom.transform(X=prds[[\"passengers\"]])\n",
    "\n",
    "\n",
    "transformer = reconciler(method=\"ols\")\n",
    "fitted_transfrom = transformer.fit(X=prds[[\"passengers\"]])\n",
    "prds[\"y_recon_ols\"] = fitted_transfrom.transform(X=prds[[\"passengers\"]])\n",
    "\n",
    "\n",
    "transformer = reconciler(method=\"wls_str\")\n",
    "fitted_transfrom = transformer.fit(X=prds[[\"passengers\"]])\n",
    "prds[\"y_recon_wls\"] = fitted_transfrom.transform(X=prds[[\"passengers\"]])\n",
    "\n",
    "\n",
    "prds.loc[prds.index.get_level_values(level=-1) == \"1961-01\"]\n",
    "# prds.loc[prds.index.get_level_values(level=-1) == \"1961-01\"]\n",
    "# .drop_duplicates(keep = \"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prds.loc[\n",
    "    prds.index.get_level_values(level=-2) != \"__total\", prds.columns[1:4]\n",
    "].copy()\n",
    "test_df.index.set_names(\"timepoints\", level=-1, inplace=True)\n",
    "\n",
    "test_df = aggregate_hierarchy(test_df, flatten_single_levels=True)\n",
    "# test_df = agg_df.fit_transform(X=test_df)\n",
    "# test_df.index.names = [\"state\", \"city\", \"airport\", None]\n",
    "# test_df.equals(prds[prds.columns[1:4]])\n",
    "(test_df - prds[prds.columns[1:4]]).apply(lambda x: x.round(6).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "df = bottom_hier_datagen(base_ts=zone1, no_bottom_nodes=20, no_levels=2)\n",
    "df.droplevel(-1).index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests And checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok for the aggregation check that\n",
    "\n",
    "    -    test that \"__total\" is named in the first index *\n",
    "    -    that the method is recognised\n",
    "    -    test that the bottom levels are unique *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should fail\n",
    "transformer = reconciler(method=\"bux\")\n",
    "fitted_transfrom = transformer.fit(X=prds[[\"passengers\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should fail\n",
    "transformer = reconciler(method=\"bu\")\n",
    "fitted_transfrom = transformer.fit(X=X[[\"var_1\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-unique bottom level names\n",
    "df = get_examples(mtype=\"pd_multiindex_hier\", as_scitype=\"Hierarchical\")\n",
    "df = df[0]\n",
    "df = aggregate_hierarchy(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = reconciler(method=\"bu\")\n",
    "fitted_transfrom = transformer.fit(X=df[[\"var_1\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in model_ids:\n",
    "#     # print(i)\n",
    "#     # print(\n",
    "#     #     mean_absolute_percentage_error(\n",
    "#     #         prds.loc[i, \"y_true\"], prds.loc[i, \"y_pred\"], symmetric=True\n",
    "#     #     )\n",
    "#     # )\n",
    "#     # print(\n",
    "#     #     mean_absolute_percentage_error(\n",
    "#     #         prds.loc[i, \"y_true\"], prds.loc[i, \"y_reco_bu\"], symmetric=True\n",
    "#     #     )\n",
    "#     # )\n",
    "#     # print(\n",
    "#     #     mean_absolute_percentage_error(\n",
    "#     #         prds.loc[i, \"y_true\"], prds.loc[i, \"y_reco_ols\"], symmetric=True\n",
    "#     #     )\n",
    "#     # )\n",
    "#     plot_series(\n",
    "#         prds.loc[i, 'y_true'],\n",
    "#         prds.loc[i, 'y_pred'],\n",
    "#         prds.loc[i, 'y_reco_bu'],\n",
    "#         prds.loc[i, 'y_reco_ols'],\n",
    "#         labels=[\"y_test\", \"y_pred\", \"y_pred_bu\", \"y_pred_ols\"],\n",
    "#     )\n",
    "\n",
    "# index = (\"LA\", \"zone1\", \"sth\")\n",
    "# \"__\".join(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we could maybe work it like this\n",
    "\n",
    "\n",
    "class (panel_forecaster)\n",
    "    \n",
    "    - fit\n",
    "    - predict\n",
    "    - train_test_temporal split?\n",
    "    - list of model specs\n",
    "\n",
    "class hierarchical_forecaster(panel_forecaster)\n",
    "    \n",
    "    Includes the aggregated levels for the panel.\n",
    "    \n",
    "    Inherits methods from above and adds g matrix methods that need information from model fits/original data\n",
    "\n",
    "    - get_g_matrix_wlsvar\n",
    "    - get_g_matrix_mint\n",
    "    - get_g_matrix_mint_shrink\n",
    "    - get_g_matrix_topdown\n",
    "    - predict generates multiindex\n",
    "\n",
    "class reconcile(Transfromer, predictions: multi-index with '__total' present, method = \"bu\"):\n",
    "\n",
    "    Inherets transfromer methods? and includes reconciliation methods that don't depend on historic/residual data.\n",
    "\n",
    "    Checks we have predicttions from hierarchical forecaster then\n",
    "\n",
    "    - fit\n",
    "    - predict, i.e. reconcile from this notebook\n",
    "    - get_s_matrix\n",
    "    - get_g_matrix_bu\n",
    "    - get_g_matrix_ols\n",
    "    - get_g_matrix_wlsstr\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1eb1c2818a749c31c67be5c4cb66d4609f775387358a3054790d55ee191d060"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('sktime-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
